{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae896427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.widgets import interact_img_repr\n",
    "from lib.plots import (\n",
    "    plot_img_repr,\n",
    "    plot_img_hist,\n",
    "    plot_linear_contrast_enhance,\n",
    "    plot_gamma_correct,\n",
    "    plot_hist_equalize,\n",
    "    plot_clahe\n",
    ")\n",
    "from lib.contrast_enhance import (\n",
    "    linear_contrast_enhance,\n",
    "    gamma_correct,\n",
    "    hist_equalize,\n",
    "    clahe,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b969539b",
   "metadata": {},
   "source": [
    "# Image representation\n",
    "\n",
    "An image is just a large table with numbers. Typically, we use 8-bits to represent each pixel value. When interpreted as unsigned integer values, these 8 bits range from 0 (darkest) to 255 (brightest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_img_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread('data/verona_gray.jpg').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5388019",
   "metadata": {},
   "source": [
    "# Image histogram\n",
    "\n",
    "A histogram counts how often each pixel value occurs in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c7206",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_bw = cv2.imread('data/verona_bw.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig_bw, ax_bw = plt.subplots()\n",
    "ax_bw.hist(img_bw.flatten(), bins=256, color='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray = cv2.imread('data/verona_gray.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig_gray, ax_gray = plt.subplots()\n",
    "ax_gray.hist(img_gray.flatten(), bins=256, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956cb049",
   "metadata": {},
   "source": [
    "A color image has multiple *channels*. A frequently-used way to represent colors is with a *red, green and blue* channel. When using `cv2.imread()`, we get a numpy array with 3 dimensions: a width, a height and 3 color channels. Note that, confusingly, OpenCV orders the color channels as *BGR* instead of the more commonly used *RGB*.\n",
    "\n",
    "To convert BGR to RGB, you can use `cv2.cvtColor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color = cv2.imread('data/verona_color.jpg')\n",
    "\n",
    "# Convert BGR to RGB\n",
    "img_color = cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Extract channels\n",
    "red_channel = img_color[..., 0]\n",
    "green_channel = img_color[..., 1]\n",
    "blue_channel = img_color[..., 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of zeros with same width and height\n",
    "zeros = np.zeros(red_channel.shape, dtype=np.uint8)\n",
    "\n",
    "# Create separate images with only red, green and blue channel\n",
    "img_red = np.stack([red_channel, zeros, zeros], axis=2)\n",
    "img_green = np.stack([zeros, green_channel, zeros], axis=2)\n",
    "img_blue = np.stack([zeros, zeros, blue_channel], axis=2)\n",
    "\n",
    "# Show images\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(3*6, 4))\n",
    "axes[0].imshow(img_red)\n",
    "axes[1].imshow(img_green)\n",
    "axes[2].imshow(img_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b1e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_color, axes_color = plt.subplots(ncols=3, figsize=(3*6, 4), sharey=True)\n",
    "\n",
    "axes_color[0].hist(img_color[..., 0].flatten(), bins=256, color='red')\n",
    "axes_color[1].hist(img_color[..., 1].flatten(), bins=256, color='green')\n",
    "axes_color[2].hist(img_color[..., 2].flatten(), bins=256, color='blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85959ce1",
   "metadata": {},
   "source": [
    "## CIELAB color space\n",
    "\n",
    "The CIELAB color space is another way to represent colors. The L-channel refers to the *lightness* of the pixel and the A and B channels are for colors.\n",
    "\n",
    "It's intended as a *perceptually uniform* space, where a given numerical change corresponds to a similar perceived change in lightness and color.\n",
    "\n",
    "There are other color spaces that implement a similar idea, e.g., YCbCr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ad81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RGB to LAB\n",
    "img_lab = cv2.cvtColor(img_color, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "# Extract channels\n",
    "l_channel = img_lab[..., 0]\n",
    "a_channel = img_lab[..., 1]\n",
    "b_channel = img_lab[..., 2]\n",
    "\n",
    "# Create separate images with only L, A and B channel\n",
    "img_l = np.stack([l_channel, zeros + 128, zeros + 128], axis=2)\n",
    "img_a = np.stack([zeros + 128, a_channel, zeros + 128], axis=2)\n",
    "img_b = np.stack([zeros + 128, zeros + 128, b_channel], axis=2)\n",
    "\n",
    "# Convert separate images to RGB for visualization\n",
    "img_l = cv2.cvtColor(img_l, cv2.COLOR_LAB2RGB)\n",
    "img_a = cv2.cvtColor(img_a, cv2.COLOR_LAB2RGB)\n",
    "img_b = cv2.cvtColor(img_b, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "# Show images\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(3*6, 4))\n",
    "axes[0].imshow(img_l)\n",
    "axes[1].imshow(img_a)\n",
    "axes[2].imshow(img_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd91709",
   "metadata": {},
   "source": [
    "# Linear contrast enhancement\n",
    "\n",
    "We can enhance contrast by simply multiplying and adding all pixel values in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e711da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.clip(np.array([250], dtype=np.uint8) + float(10), 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('data/verona_color.jpg')\n",
    "img = img[..., ::-1]  # Shorthand for cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "new_img = linear_contrast_enhance(img, alpha=2.0, beta=10)\n",
    "\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9370c48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_linear_contrast_enhance(img, alpha=3.0, beta=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d866e",
   "metadata": {},
   "source": [
    "# Gamma correction\n",
    "\n",
    "Our visual system is more sensitive to contrasts in the dark regions, so we might want to enhance particularly those contrasts. The \"sensitivity\" of our visual system can be approximated by a gamma function.\n",
    "\n",
    "This is used, for example, to transform the output signal of a camera such that more bits are used for the darker regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21803eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = gamma_correct(img, gamma=0.5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f61789",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gamma_correct(img, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b5907",
   "metadata": {},
   "source": [
    "# Histogram equalization\n",
    "\n",
    "With histogram equalization, to which values we should map existing pixel values such that the histogram will be more uniformly distributed. This can be done by using the CDF of the histogram as a look-up table.\n",
    "\n",
    "Note that we apply histogram equalization only to the lightness channel of the image, as this is what our eyes are most sensitive to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = hist_equalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa24cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_equalize(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1f276",
   "metadata": {},
   "source": [
    "# Contrast-limited adaptive histogram equalization (CLAHE)\n",
    "\n",
    "Instead of remapping all pixels in the same way, we can split up the image into several regions and apply histogram equalization per region. To avoid blocks in the image, we apply the CDFs of the neighboring regions and calculate a weighted average, with weights proportional to distance to neighboring center pixels. This is **adaptive histogram equalization**.\n",
    "\n",
    "To avoid noise in near-constant regions, we apply contrast limiting to the histograms before computing the CDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff63ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_limit: max peak in histogram, relative to the height of a uniform histogram\n",
    "# grid_size: the number of tiles per row and column\n",
    "new_img = clahe(img, clip_limit=2.0, grid_size=8)\n",
    "\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clahe(img, clip_limit=2.0, grid_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maibi_cv",
   "language": "python",
   "name": "maibi_cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
